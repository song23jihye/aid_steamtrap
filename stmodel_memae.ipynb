{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP2VLHE9e7qhmwUDA7fFrQa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/song23jihye/aid_steamtrap/blob/master/stmodel_memae.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIOZTA-yFc7T",
        "outputId": "4ccebbf0-312e-4213-c58d-59d6598b6b3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#모듈 사용\n",
        "\n",
        "import os\n",
        "os.chdir('/content/drive/MyDrive/steam_trap_project')"
      ],
      "metadata": {
        "id": "eCpTJSevGHH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import copy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from pylab import rcParams\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import math\n",
        "from torch.nn.parameter import Parameter\n",
        "#from matplotlib import rcb\n",
        "\n",
        "from torchvision.transforms import transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch import nn, optim\n",
        "from PIL import Image\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "from models.entropy_loss import EntropyLossEncap\n",
        "from models.memae_2dmlp_conv import AutoEncoderCov2DMem\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from models.memory_module_series import MemModule\n",
        "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
        "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
        "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
        "rcParams['figure.figsize'] = 15, 8"
      ],
      "metadata": {
        "id": "R4yZ7T9jGJxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.fft import fft\n",
        "\n",
        "# Spectral entropy 계산 함수\n",
        "def spectral_entropy(signal, normalize=False):\n",
        "    freq_domain = fft(signal)\n",
        "    magnitude = np.abs(freq_domain)\n",
        "    if normalize:\n",
        "        magnitude = magnitude / np.sum(magnitude)\n",
        "    entropy = -np.sum(magnitude*np.log2(magnitude + np.finfo(float).eps))\n",
        "    return entropy\n",
        "\n",
        "# # 엔트로피 계산\n",
        "# train_entropies = []\n",
        "# test_entropies = []\n",
        "\n",
        "# # Train 데이터의 스펙트럼 엔트로피 계산\n",
        "# for i in range(train_df.shape[0]):\n",
        "#     row_data = train_df.iloc[i].values\n",
        "#     entropy = spectral_entropy(row_data, normalize=True)\n",
        "#     train_entropies.append(entropy)\n",
        "\n",
        "# # Test 데이터의 스펙트럼 엔트로피 계산\n",
        "# for i in range(test_df.shape[0]):\n",
        "#     row_data = test_df.iloc[i].values\n",
        "#     entropy = spectral_entropy(row_data, normalize=True)\n",
        "#     test_entropies.append(entropy)\n",
        "\n",
        "# average_entropy = np.mean(train_entropies)\n",
        "\n",
        "# 데이터셋 생성 함수\n",
        "def create_dataset(df):\n",
        "    sequences = df.astype(np.float32).to_numpy().tolist()\n",
        "    dataset = [torch.tensor(s).unsqueeze(1).float() for s in sequences]\n",
        "    n_seq, seq_len, n_features = torch.stack(dataset).shape\n",
        "    return dataset, seq_len, n_features\n",
        "\n",
        "# # Train 및 Test 데이터셋 생성\n",
        "# train_dataset, seq_len, n_features = create_dataset(train_df)\n",
        "# test_normal_dataset, _, _ = create_dataset(test_df)\n",
        "\n",
        "class Encoder1(nn.Module):\n",
        "    def __init__(self, seq_len, n_features, embedding_dim=128,dropout_prob=0.0):\n",
        "        super(Encoder1, self).__init__()\n",
        "        self.seq_len = seq_len\n",
        "        self.n_features = n_features\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = 2 * embedding_dim\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "        # You might need to adjust the sizes here to fit your needs.\n",
        "        self.fc1 = nn.Linear(self.n_features, self.hidden_dim) #seq_len * n_features\n",
        "        self.fc2 = nn.Linear(self.hidden_dim, self.embedding_dim * n_features)\n",
        "\n",
        "    def forward(self, x):\n",
        "        try:\n",
        "            B, S, E = x.shape  # B = Batch size, S = Sequence length, E = Feature dim\n",
        "        except:\n",
        "            B = 1\n",
        "        x = x.view(B, -1)  # Flatten the input: shape becomes (B, S*E)\n",
        "        x = x.unsqueeze(-1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        # Here, we reshape the output to mimic your original Encoder's output shape\n",
        "        x = x.view(B, self.seq_len, self.embedding_dim)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Modified Decoder with Fully Connected Layers\n",
        "class Decoder1(nn.Module):\n",
        "    def __init__(self, seq_len, input_dim=128, n_features=1):\n",
        "        super(Decoder1, self).__init__()\n",
        "        self.seq_len = seq_len\n",
        "        self.input_dim = input_dim  # Embedding dim from Encoder\n",
        "        self.hidden_dim = 2 * input_dim  # Mimicking original architecture\n",
        "        self.n_features = n_features  # Usually 1, representing the reconstructed feature\n",
        "        self.fc1 = nn.Linear(self.input_dim * seq_len, self.hidden_dim)\n",
        "        self.fc2 = nn.Linear(self.hidden_dim, seq_len * n_features)\n",
        "        self.output_layer = nn.Linear(seq_len * n_features, seq_len * n_features)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,E,G = x.shape\n",
        "        #x = x.repeat(1,self.seq_len, self.n_features)\n",
        "        #x = x.reshape((B, self.input_dim,1))\n",
        "        x = x.reshape(B, -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.output_layer(x)\n",
        "\n",
        "        # Reshape to mimic your original Decoder's output shape\n",
        "        x = x.view(B, self.seq_len, self.n_features)\n",
        "        return x\n",
        "\n",
        "\n",
        "# 예측 모델 정의\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self, seq_len, n_features, embedding_dim=64):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        self.encoder = Encoder1(seq_len, n_features, embedding_dim)\n",
        "        self.decoder = Decoder1(seq_len, embedding_dim, n_features)\n",
        "        self.mem_rep = MemModule(mem_dim=500, fea_dim=1280, shrink_thres=0.0025)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        res_mem = self.mem_rep(x)\n",
        "        x = res_mem['output']\n",
        "        att = res_mem['att']\n",
        "        x = self.decoder(x)\n",
        "        return x, att\n",
        "\n",
        "class TimeSeriesDataset(Dataset):\n",
        "    def __init__(self, sequences,img,labels=None,transform=None):\n",
        "        self.sequences = sequences\n",
        "        self.transform = transform\n",
        "        self.img = img\n",
        "        self.labels = labels\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img = self.img[index]\n",
        "        labels = self.labels[index]\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return self.sequences[index],img,labels\n",
        "\n",
        "mean = [0.5]\n",
        "std = [0.5]\n",
        "\n",
        "# 데이터로더 생성 함수\n",
        "def create_train_data_loader(df, batch_size,img,labels):\n",
        "    sequences = df.astype(np.float32).to_numpy().tolist()\n",
        "    dataset = TimeSeriesDataset([torch.tensor(s).unsqueeze(1).float() for s in sequences],img,labels,transform=transforms.Compose(\n",
        "                                    [\n",
        "                                     transforms.ToTensor(),\n",
        "                                     transforms.Normalize(mean=mean, std=std)]))\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=False,drop_last=False)\n",
        "\n",
        "def create_test_data_loader(df,batch_size,img,labels):\n",
        "    sequences = df.astype(np.float32).to_numpy().tolist()\n",
        "    dataset = TimeSeriesDataset([torch.tensor(s).unsqueeze(1).float() for s in sequences],img,labels,transform=transforms.Compose(\n",
        "                                    [\n",
        "                                     transforms.ToTensor(),\n",
        "                                     transforms.Normalize(mean=mean, std=std)]))\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# 모델 및 데이터로더 생성\n",
        "train_dataloader = create_train_data_loader(train_df, 1)\n",
        "test_dataloader = create_test_data_loader(test_df, 1)\n",
        "\n",
        "# 모델 및 가중치 로드\n",
        "model_series = Autoencoder(seq_len=1280, n_features=1, embedding_dim=400).to(device)\n",
        "model_series = torch.load('/content/drive/MyDrive/steam_trap_project/mdlpth/mae_vibration_series_model.pth')\n",
        "model_series.to(device)\n",
        "model_series.eval()\n",
        "\n",
        "# 예측 함수\n",
        "def predict(model_series,model_img,dataloader):\n",
        "    entropy_loss_weight=0.0002\n",
        "    predictions, losses = [], []\n",
        "    tr_recon_loss_func = nn.MSELoss().to(device)  ###nn.MSELoss().to(device)#\n",
        "    tr_entropy_loss_func = EntropyLossEncap().to(device)\n",
        "    criterion = nn.MSELoss().to(device)##nn.MSELoss().to(device)\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (seq_true,frames,labels) in enumerate(dataloader):\n",
        "            frames = frames.to(device)\n",
        "            frq_entropy = train_entropies[batch_idx]\n",
        "            frq_entropy = (frq_entropy+average_entropy)/2\n",
        "            seq_true = seq_true.to(device)\n",
        "            labels = labels.to(device).float()\n",
        "            #print('frames:',frames.shape)\n",
        "            recon_res = model_img(frames)\n",
        "            seq_pred, att_w1 = model_series(seq_true)\n",
        "            recon_frames = recon_res['output']\n",
        "            r_frame = recon_frames - frames\n",
        "            sp_error_map = torch.sum(r_frame**2, dim=1)**0.5\n",
        "            s = sp_error_map.size()\n",
        "            sp_error_vec = sp_error_map.view(s[0], -1)\n",
        "            recon_error_frame = torch.mean(sp_error_vec, dim=-1)\n",
        "            recon_error_series = tr_recon_loss_func(seq_pred, seq_true)\n",
        "            recon_error = frq_entropy*recon_error_frame + opt.series_w*recon_error_series\n",
        "            #predictions.append(img_pred.cpu().numpy().flatten())\n",
        "            losses.append(recon_error.item())\n",
        "    return predictions, losses\n",
        "\n",
        "# 예측 및 결과 분석\n",
        "_, losses = predict(model_series,model_img, train_dataloader)\n",
        "# 결과 분석 (예: 시각화, 분류 등)\n",
        "sns.distplot(losses, bins=150, kde=True);"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "id": "sCrtRiUaGC42",
        "outputId": "619debaa-3e95-4d94-8f38-dd741a004093"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-b195e1a2175d>\u001b[0m in \u001b[0;36m<cell line: 149>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;31m# 모델 및 데이터로더 생성\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m \u001b[0mtrain_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_train_data_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0mtest_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_test_data_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(losses)\n",
        "print(df.describe())\n",
        "df.plot.box(title=\"Box Chart\")\n",
        "plt.grid(linestyle=\"--\", alpha=0.3)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5cxaZCbVGEyc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}